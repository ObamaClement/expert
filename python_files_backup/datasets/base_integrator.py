from abc import ABC, abstractmethod
from sqlalchemy.orm import Session

class BaseIntegrator(ABC):
    """
    Classe de base abstraite (blueprint) pour tous les int√©grateurs de datasets.
    
    Elle impose une structure ETL (Extract, Transform, Load) coh√©rente pour
    garantir que chaque script d'importation fonctionne de la m√™me mani√®re.
    """

    def __init__(self, db_session: Session, dataset_path: str):
        """
        Initialise l'int√©grateur avec une session de base de donn√©es et le chemin
        vers le dataset.
        
        :param db_session: La session SQLAlchemy pour interagir avec la BDD.
        :param dataset_path: Le chemin vers le dossier ou le fichier du dataset.
        """
        self.db = db_session
        self.path = dataset_path
        print(f"--- Initialisation de {self.__class__.__name__} ---")
        print(f"Source des donn√©es : {self.path}")

    @abstractmethod
    def extract(self):
        """
        √âtape d'Extraction (E) : Lire les donn√©es depuis la source.
        
        Cette m√©thode DOIT √™tre impl√©ment√©e par chaque sous-classe.
        Elle doit retourner un it√©rateur qui produit des lots (chunks) de donn√©es
        (par exemple, un TextFileReader de pandas).
        """
        pass

    @abstractmethod
    def transform(self, data_chunk: any):
        """
        √âtape de Transformation (T) : Nettoyer, mapper et pr√©parer les donn√©es.
        
        Cette m√©thode DOIT √™tre impl√©ment√©e par chaque sous-classe.
        Elle prend un lot de donn√©es extraites et retourne une liste d'objets
        SQLAlchemy pr√™ts √† √™tre ins√©r√©s.
        """
        pass

    @abstractmethod
    def load(self, transformed_data: list):
        """
        √âtape de Chargement (L) : Ins√©rer les donn√©es transform√©es en BDD.
        
        Cette m√©thode DOIT √™tre impl√©ment√©e par chaque sous-classe.
        """
        pass

    def run(self):
        """
        Orchestre le processus ETL complet.
        
        Cette m√©thode est d√©j√† impl√©ment√©e et ne devrait pas √™tre modifi√©e.
        Elle appelle successivement extract, transform, et load pour chaque lot.
        """
        print(f"\nüöÄ D√©marrage du processus ETL pour {self.__class__.__name__}...")
        
        try:
            extracted_data_iterator = self.extract()
            
            total_items_loaded = 0
            chunk_count = 0
            for chunk in extracted_data_iterator:
                chunk_count += 1
                print(f"  [{chunk_count}] Extraction d'un lot de {len(chunk)} lignes.")
                
                transformed_chunk = self.transform(chunk)
                
                if transformed_chunk:
                    print(f"    -> Transformation r√©ussie : {len(transformed_chunk)} objets pr√™ts √† √™tre charg√©s.")
                    self.load(transformed_chunk)
                    total_items_loaded += len(transformed_chunk)
                else:
                    print("    -> Aucun nouvel objet √† charger dans ce lot.")
            
            print(f"\n‚ú® Processus ETL termin√©. {total_items_loaded} objets uniques charg√©s au total.")
        except FileNotFoundError:
            print(f"‚ùå ERREUR: Le fichier ou dossier du dataset n'a pas √©t√© trouv√© √† l'emplacement : {self.path}")
        except Exception as e:
            print(f"‚ùå ERREUR inattendue pendant le processus ETL : {e}")
            # En production, on utiliserait un logger plus sophistiqu√©.