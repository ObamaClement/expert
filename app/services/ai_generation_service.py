#=== Fichier: ./app/services/ai_generation_service.py ===

import logging
import requests
import json
import time
import uuid
import re
import traceback
from typing import Dict, Any, List, Tuple, Optional, Union
from enum import Enum

from sqlalchemy.orm import Session, joinedload
from .. import models, schemas
from ..config import settings
from ..core.prompts.exam_prompts import exam_prompt_builder

# ==============================================================================
# CONFIGURATION DU LOGGER "AI-KERNEL" (Niveau Expert / Debugging)
# ==============================================================================
# Ce logger est configur√© pour capturer absolument tout ce qui entre et sort.
# Il est distinct du logger principal pour permettre un filtrage fin.
logger = logging.getLogger("ai_kernel")
logger.setLevel(logging.DEBUG)

if not logger.handlers:
    handler = logging.StreamHandler()
    # Format enrichi : Date - Logger - Niveau - Fichier:Ligne - Message
    formatter = logging.Formatter(
        '%(asctime)s - [AI-KERNEL] - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# ==============================================================================
# CONSTANTES ET CONFIGURATION
# ==============================================================================

OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"

# Mod√®le choisi : Mistral 7B Instruct (Bon rapport qualit√©/prix/performance pour le roleplay)
# Alternatives test√©es : 'openai/gpt-4o-mini', 'anthropic/claude-3-haiku'
MODEL_NAME = "mistralai/devstral-2512:free" 

# Configuration de r√©silience
MAX_RETRIES_NETWORK = 3    # Tentatives en cas d'√©chec de connexion
MAX_RETRIES_LOGIC = 2      # Tentatives en cas de JSON malform√©
TIMEOUT_SECONDS = 60       # Timeout strict pour ne pas bloquer le worker

class AiTaskType(Enum):
    """√ânum√©ration des types de t√¢ches pour le tagging des logs."""
    CHAT_PATIENT = "CHAT_PATIENT"
    EXAM_GENERATION = "EXAM_GENERATION"
    EVALUATION = "EVALUATION"
    HINT_GENERATION = "HINT_GENERATION"

# ==============================================================================
# UTILITAIRES DE NETTOYAGE ET VALIDATION
# ==============================================================================

def _clean_json_string(json_str: str, trace_id: str = "N/A") -> str:
    """
    Nettoie une cha√Æne JSON brute renvoy√©e par un LLM.
    Les LLM aiment bien entourer le JSON de balises Markdown ```json ... ``` 
    ou ajouter du texte avant/apr√®s ("Voici le rapport : ...").
    
    :param json_str: La cha√Æne brute re√ßue de l'API.
    :param trace_id: ID de tra√ßabilit√© pour les logs.
    :return: Une cha√Æne contenant uniquement le JSON potentiel.
    """
    original_len = len(json_str)
    
    # 1. Supprimer les balises Markdown (classique)
    if "```" in json_str:
        logger.debug(f"   üßπ [{trace_id}] D√©tection de blocs Markdown, nettoyage en cours...")
        # Regex pour capturer le contenu entre ```json et ``` ou juste ``` et ```
        pattern = r"```(?:json)?\s*(.*?)\s*```"
        match = re.search(pattern, json_str, re.DOTALL)
        if match:
            json_str = match.group(1)
            logger.debug(f"   üßπ [{trace_id}] Bloc Markdown extrait.")
    
    # 2. Trouver la premi√®re accolade ouvrante et la derni√®re fermante
    # Cela √©limine tout le texte introductif ("Sure, here is the JSON:")
    start = json_str.find("{")
    end = json_str.rfind("}")
    
    if start != -1 and end != -1:
        if start > 0 or end < len(json_str) - 1:
            logger.debug(f"   üßπ [{trace_id}] Rognage du texte autour du JSON (Indices: {start} √† {end})")
            json_str = json_str[start : end + 1]
    
    final_len = len(json_str)
    if final_len != original_len:
        logger.debug(f"   ‚ú® [{trace_id}] Nettoyage termin√© : {original_len} -> {final_len} chars")
        
    return json_str.strip()

def _validate_exam_json_structure(data: Dict[str, Any], trace_id: str) -> bool:
    """
    V√©rifie que le JSON d'un examen contient les cl√©s minimales requises.
    
    :param data: Le dictionnaire pars√©.
    :return: True si valide, False sinon.
    """
    required_keys = ["rapport_complet", "conclusion"]
    missing = [k for k in required_keys if k not in data]
    
    if missing:
        logger.error(f"   ‚ùå [{trace_id}] Validation JSON √©chou√©e. Cl√©s manquantes : {missing}")
        return False
    
    # V√©rification du contenu non vide
    if not data.get("rapport_complet") or len(str(data["rapport_complet"])) < 10:
        logger.warning(f"   ‚ö†Ô∏è [{trace_id}] Validation suspecte : 'rapport_complet' semble trop court.")
        # On laisse passer mais on logue le warning
        
    return True

# ==============================================================================
# NOYAU D'APPEL API (CORE)
# ==============================================================================

def _call_openrouter_api(
    input_data: Union[str, List[Dict[str, str]]], 
    json_mode: bool = False,
    temperature: float = 0.7,
    task_type: AiTaskType = AiTaskType.CHAT_PATIENT,
    max_tokens: int = 1500
) -> Any:
    """
    Fonction noyau (Core) pour appeler l'API LLM.
    Elle est con√ßue pour √™tre une bo√Æte noire totalement transparente via les logs.
    
    :param input_data: Le prompt (str) ou la liste de messages (list).
    :param json_mode: Force le mod√®le √† produire du JSON et active le validateur.
    :param temperature: Cr√©ativit√© (0.0 = Rigide, 1.0 = Folie).
    :param task_type: Type de t√¢che pour le logging.
    """
    trace_id = f"AI-{str(uuid.uuid4())[:6].upper()}"
    
    logger.info(f"‚ö° [{trace_id}] D√âBUT TRANSACTION API | T√¢che: {task_type.value} | Mode JSON: {json_mode}")
    logger.debug(f"   [{trace_id}] Config: Temp={temperature}, MaxTokens={max_tokens}, Model={MODEL_NAME}")

    # 1. Normalisation du Payload
    messages = []
    if isinstance(input_data, str):
        messages = [{"role": "user", "content": input_data}]
    else:
        messages = input_data

    # ==========================================================================
    # üîç PROMPT DUMP - LOGGING EXTENSIF
    # ==========================================================================
    logger.debug(f"\n{'='*40} [{trace_id}] PROMPT ENVOY√â {'='*40}")
    for i, msg in enumerate(messages):
        role = msg.get('role', 'unknown').upper()
        content = msg.get('content', '')
        # Affichage s√©curis√© (tronqu√© si trop long pour la console, mais on garde assez pour debug)
        display_content = content if len(content) < 2000 else f"{content[:2000]}... [TRONQU√â {len(content)-2000} chars]"
        logger.debug(f"[{i}] {role}:\n{display_content}\n{'-'*20}")
    logger.debug(f"{'='*100}\n")
    # ==========================================================================

    headers = {
        "Authorization": f"Bearer {settings.OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://expert-cmck.onrender.com",
        "X-Title": "STI Medical Expert System"
    }
    
    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
    }

    if json_mode:
        # Hint pour les mod√®les compatibles OpenAI
        payload["response_format"] = {"type": "json_object"}

    # 2. Boucle de Tentatives (Retry Loop)
    attempt = 0
    
    while attempt < MAX_RETRIES_NETWORK:
        attempt += 1
        start_time = time.time()
        
        try:
            if attempt > 1:
                logger.warning(f"   üîÑ [{trace_id}] Tentative r√©seau {attempt}/{MAX_RETRIES_NETWORK}...")
                # Backoff exponentiel (2s, 4s, 8s...)
                sleep_time = 2 ** attempt
                time.sleep(sleep_time)

            logger.debug(f"   üöÄ [{trace_id}] Envoi requ√™te POST vers {OPENROUTER_API_URL}...")
            
            response = requests.post(
                OPENROUTER_API_URL, 
                headers=headers, 
                data=json.dumps(payload), 
                timeout=TIMEOUT_SECONDS
            )
            
            latency = time.time() - start_time
            
            # --- Analyse de la R√©ponse HTTP ---
            if response.status_code == 200:
                response_data = response.json()
                
                # Metrics d'utilisation
                usage = response_data.get('usage', {})
                p_tok = usage.get('prompt_tokens', 0)
                c_tok = usage.get('completion_tokens', 0)
                logger.info(f"   ‚úÖ [{trace_id}] Succ√®s HTTP 200 | Latence: {latency:.2f}s | Tokens: {p_tok} in / {c_tok} out")

                # Extraction du contenu
                try:
                    if not response_data.get('choices'):
                        raise ValueError("Liste 'choices' vide dans la r√©ponse API")

                    choice = response_data['choices'][0]
                    raw_content = choice['message']['content']
                    finish_reason = choice.get('finish_reason', 'unknown')
                    
                    if finish_reason == 'length':
                        logger.warning(f"   ‚ö†Ô∏è [{trace_id}] Attention: La r√©ponse a √©t√© tronqu√©e (max_tokens atteint). Le JSON risque d'√™tre cass√©.")

                    # ==========================================================
                    # üîç RESPONSE DUMP
                    # ==========================================================
                    logger.debug(f"\n{'='*40} [{trace_id}] R√âPONSE BRUTE IA {'='*40}")
                    logger.debug(f"{raw_content}")
                    logger.debug(f"{'='*100}\n")
                    # ==========================================================

                    # Traitement JSON si requis
                    if json_mode:
                        cleaned_content = _clean_json_string(raw_content, trace_id)
                        try:
                            parsed_json = json.loads(cleaned_content)
                            logger.info(f"   ‚úÖ [{trace_id}] JSON pars√© et valid√© techniquement.")
                            return parsed_json
                        except json.JSONDecodeError as je:
                            logger.error(f"   ‚ùå [{trace_id}] √âchec du parsing JSON.")
                            logger.error(f"      Source nettoy√©e : {cleaned_content}")
                            logger.error(f"      Erreur Python : {str(je)}")
                            
                            # Logique de Retry "Logique" (si on n'a pas √©puis√© les essais)
                            # On pourrait relancer l'appel en disant √† l'IA qu'elle s'est tromp√©e,
                            # mais pour ce prototype, on l√®ve l'erreur pour le catch global.
                            raise ValueError(f"L'IA n'a pas produit un JSON valide : {str(je)}")
                    
                    # Mode texte simple
                    return raw_content

                except (KeyError, IndexError, ValueError) as e:
                    logger.error(f"   ‚ùå [{trace_id}] Erreur structurelle r√©ponse API : {str(e)}")
                    # On ne retry pas une erreur de structure interne, c'est probablement fatal
                    raise e

            elif response.status_code == 429:
                logger.warning(f"   ‚ö†Ô∏è [{trace_id}] Rate Limit atteint (429). Pause forc√©e.")
                time.sleep(5) # Pause fixe
                continue 
            
            elif response.status_code >= 500:
                logger.error(f"   üî• [{trace_id}] Erreur Serveur IA ({response.status_code}).")
                logger.debug(f"      Body: {response.text}")
                continue
            
            else:
                # Erreur client (400, 401, 403) -> Pas de retry
                logger.critical(f"   ‚õî [{trace_id}] Erreur Client {response.status_code}.")
                logger.critical(f"      R√©ponse: {response.text}")
                response.raise_for_status()

        except requests.exceptions.RequestException as e:
            logger.error(f"   üåê [{trace_id}] Exception R√©seau : {str(e)}")
            continue

    # Si on sort de la boucle, c'est l'√©chec total
    logger.critical(f"   üíÄ [{trace_id}] √âCHEC TOTAL apr√®s {MAX_RETRIES_NETWORK} tentatives r√©seaux.")
    
    if json_mode:
        return {} 
    return "(Erreur technique : Le service d'IA est injoignable pour le moment.)"


# ==============================================================================
# SERVICES M√âTIERS (Business Logic)
# ==============================================================================

def generate_patient_reply_chat(messages: List[Dict[str, str]]) -> str:
    """
    G√©n√®re la r√©plique du patient (Mode Chat).
    
    Cette fonction est appel√©e par le PatientActorService.
    Elle privil√©gie une temp√©rature √©lev√©e pour la vari√©t√© et le naturel.
    """
    try:
        response = _call_openrouter_api(
            input_data=messages,
            json_mode=False,
            temperature=0.85, # Cr√©atif
            task_type=AiTaskType.CHAT_PATIENT,
            max_tokens=300 # R√©ponses courtes (patient)
        )
        
        if isinstance(response, str):
            return response
        return "..."
    except Exception as e:
        logger.error(f"Erreur dans generate_patient_reply_chat: {e}")
        return "(Silence...)"


def generate_exam_result(
    case: models.ClinicalCase, 
    session_history: List[str], 
    exam_name: str,
    exam_justification: str = "Non sp√©cifi√©e"
) -> Dict[str, Any]:
    """
    G√©n√®re un r√©sultat d'examen m√©dical structur√©.
    
    C'est le C≈íUR de la fonctionnalit√© d'examen.
    Elle utilise le `ExamPromptBuilder` pour cr√©er un prompt contextuel hyper-pr√©cis.
    """
    logger.info(f"üî¨ [AI-LAB] Demande g√©n√©ration examen : '{exam_name}'")
    
    # 1. Pr√©paration des donn√©es pour le Builder
    # Conversion du mod√®le SQLAlchemy en dict simpliste pour le builder
    case_data = {
        "pathologie_principale": {
            "nom_fr": case.pathologie_principale.nom_fr if case.pathologie_principale else "Inconnue"
        },
        "niveau_gravite": case.niveau_difficulte,
        "donnees_paracliniques": case.donnees_paracliniques,
        "description": case.pathologie_principale.description if case.pathologie_principale else "",
        "physiopathologie": case.pathologie_principale.physiopathologie if case.pathologie_principale else ""
    }
    
    # Extraction sommaire du persona depuis l'historique ou donn√©es par d√©faut
    # (Id√©alement, on devrait passer le persona complet, mais ici on fait simple pour l'√¢ge/sexe)
    patient_persona = {
        "age": "Adulte (selon dossier)", # Sera affin√© si le texte du cas contient l'√¢ge
        "genre": "Non sp√©cifi√©"
    }
    
    exam_req = {
        "name": exam_name,
        "type": "tous", # Le builder d√©duira le type (bio/imag)
        "justification": exam_justification
    }

    # 2. Construction du Prompt via le Builder d√©di√©
    prompt = exam_prompt_builder.build_prompt(
        case_data=case_data,
        exam_request=exam_req,
        patient_persona=patient_persona
    )

    # 3. Appel IA avec logique de retry sur le format JSON
    logic_attempts = 0
    final_result = None
    
    while logic_attempts < MAX_RETRIES_LOGIC:
        logic_attempts += 1
        
        try:
            result = _call_openrouter_api(
                input_data=prompt,
                json_mode=True,
                temperature=0.2, # Tr√®s strict pour des donn√©es m√©dicales
                task_type=AiTaskType.EXAM_GENERATION,
                max_tokens=1000
            )
            
            # 4. Validation M√©tier
            if isinstance(result, dict) and _validate_exam_json_structure(result, f"EXAM-{logic_attempts}"):
                final_result = result
                break # Succ√®s !
            else:
                logger.warning(f"   ‚ö†Ô∏è [AI-LAB] Tentative {logic_attempts}: JSON re√ßu mais invalide structurellement.")
                # On retente (l'al√©atoire de la temp√©rature peut aider √† corriger)
        
        except Exception as e:
            logger.error(f"   ‚ùå [AI-LAB] Tentative {logic_attempts} √©chou√©e : {str(e)}")
            # On retente
            
    # 5. Gestion du Fallback (Si √©chec apr√®s retries)
    if not final_result:
        logger.critical(f"   üíÄ [AI-LAB] √âchec d√©finitif de g√©n√©ration de l'examen '{exam_name}'. Utilisation du fallback.")
        return {
            "type_resultat": "erreur",
            "rapport_complet": f"Erreur technique : Impossible de g√©n√©rer le rapport pour {exam_name}. Veuillez contacter le support.",
            "conclusion": "Examen non r√©alis√©."
        }
    
    # 6. Post-traitement (optionnel)
    # On pourrait ajouter ici des v√©rifications de s√©curit√© (mots interdits, etc.)
    
    logger.info(f"   üéâ [AI-LAB] R√©sultat g√©n√©r√© avec succ√®s. Conclusion : {final_result.get('conclusion', '')[:50]}...")
    return final_result


def evaluate_final_submission(
    db: Session,
    case: models.ClinicalCase,
    submission: schemas.simulation.SubmissionRequest,
    session_history: list
) -> Tuple[schemas.simulation.EvaluationResult, str, str]:
    """
    Le Juge S√©mantique. √âvalue la performance de l'√©tudiant en comparant
    ses r√©ponses textuelles avec la v√©rit√© structur√©e de la base de donn√©es.
    """
    eval_id = f"JUDGE-{str(uuid.uuid4())[:6]}"
    logger.info(f"‚öñÔ∏è [{eval_id}] D√©marrage √©valuation S√âMANTIQUE")

    # 1. R√©cup√©ration de la V√âRIT√â TERRAIN (Ce qu'il fallait trouver)
    # -------------------------------------------------------------------------
    logger.debug(f"   [{eval_id}] Chargement de la v√©rit√© terrain depuis la BDD...")
    
    # Pathologie correcte
    correct_pathology_name = case.pathologie_principale.nom_fr
    
    # Traitements corrects (liste des m√©dicaments li√©s √† la pathologie)
    correct_treatments_objs = db.query(models.TraitementPathologie).options(
        joinedload(models.TraitementPathologie.medicament)
    ).filter(
        models.TraitementPathologie.pathologie_id == case.pathologie_principale_id
    ).all()
    
    # On construit une liste lisible pour l'IA : "Nom (Type - Ligne)"
    correct_treatments_list = []
    for t in correct_treatments_objs:
        med_name = t.medicament.nom_commercial or t.medicament.dci
        details = []
        if t.type_traitement: details.append(t.type_traitement)
        if t.ligne_traitement: details.append(f"{t.ligne_traitement}√®re ligne")
        
        info_str = f"- {med_name}"
        if details:
            info_str += f" ({', '.join(details)})"
        correct_treatments_list.append(info_str)
    
    correct_treatments_str = "\n".join(correct_treatments_list) if correct_treatments_list else "Pas de traitement sp√©cifique d√©fini en base (se r√©f√©rer aux guidelines)."

    logger.debug(f"   [{eval_id}] V√©rit√© Patho: {correct_pathology_name}")
    logger.debug(f"   [{eval_id}] V√©rit√© Traitements: {len(correct_treatments_list)} items")

    # 2. R√©cup√©ration de la SOUMISSION √âTUDIANT (Texte libre)
    # -------------------------------------------------------------------------
    student_diagnosis_text = submission.diagnosed_pathology_text
    student_treatment_text = submission.prescribed_treatment_text
    
    logger.debug(f"   [{eval_id}] Input √âtudiant Patho: '{student_diagnosis_text}'")
    logger.debug(f"   [{eval_id}] Input √âtudiant Traitement: '{student_treatment_text[:50]}...'")

    # 3. Formatage de l'historique (Preuves de la d√©marche)
    # -------------------------------------------------------------------------
    # On tronque pour ne pas d√©passer la fen√™tre de contexte du LLM
    history_str = json.dumps(session_history, indent=2, ensure_ascii=False)
    if len(history_str) > 5000:
        history_str = history_str[:5000] + "\n... [HISTORIQUE TRONQU√â] ..."

    # 4. Construction du PROMPT DU JURY (Comparaison S√©mantique)
    # -------------------------------------------------------------------------
    prompt = f"""
TU ES UN PROFESSEUR DE M√âDECINE EXPERT (JURY D'EXAMEN).
Ta mission est d'√©valuer la pertinence clinique de la r√©ponse d'un √©tudiant.
Tu dois faire une COMPARAISON S√âMANTIQUE entre la v√©rit√© terrain et la r√©ponse de l'√©tudiant.

--- 1. LE DIAGNOSTIC ---
V√âRIT√â (Attendu) : "{correct_pathology_name}"
R√âPONSE √âTUDIANT : "{student_diagnosis_text}"

Instruction de notation Diagnostic :
- 10/10 : Diagnostic exact ou synonyme m√©dical parfait (ex: "Infarctus" = "IDM").
- 7-9/10 : Diagnostic tr√®s proche ou incomplet (ex: "Paludisme" au lieu de "Paludisme grave").
- 4-6/10 : Bonne famille de maladie mais impr√©cis (ex: "Infection virale" pour "Grippe").
- 0-3/10 : Diagnostic faux ou dangereux.

--- 2. LE TRAITEMENT ---
V√âRIT√â (Recommand√©) :
{correct_treatments_str}

R√âPONSE √âTUDIANT :
"{student_treatment_text}"

Instruction de notation Th√©rapeutique :
- Analyse si l'√©tudiant a cit√© les mol√©cules cl√©s (DCI ou nom commercial).
- 5/5 : Traitement complet et adapt√©.
- 3-4/5 : Mol√©cule principale pr√©sente mais incomplet.
- 0-2/5 : Traitement inefficace ou dangereux.

--- 3. LA D√âMARCHE CLINIQUE (HISTORIQUE) ---
Parcours de l'√©tudiant :
{history_str}

Instruction de notation D√©marche :
- 5/5 : Questions pertinentes, examens justifi√©s, logique claire.
- 0-2/5 : Questions au hasard, examens inutiles ("p√™che aux infos").

--- FORMAT DE SORTIE ATTENDU (JSON) ---
{{
  "score_diagnostic": float,  // Note sur 10
  "score_therapeutique": float, // Note sur 5
  "score_demarche": float,      // Note sur 5
  "feedback_global": "Analyse p√©dagogique d√©taill√©e. Explique pourquoi le diagnostic est bon/mauvais par rapport √† la v√©rit√©. Commente le choix des m√©dicaments.",
  "recommendation_next_step": "Conseil court (ex: 'Revoir la pharmacologie des antipalud√©ens')."
}}
"""

    # 5. Appel IA
    # -------------------------------------------------------------------------
    logger.info(f"   üöÄ [{eval_id}] Envoi du dossier au jury (LLM)...")
    
    # On utilise _call_openrouter_api (assurez-vous qu'elle est bien d√©finie dans le fichier complet)
    eval_json = _call_openrouter_api(
        input_data=prompt,
        json_mode=True,
        temperature=0.2, # Faible temp√©rature pour une notation objective
        task_type=AiTaskType.EVALUATION
    )

    # 6. Parsing et Validation du R√©sultat
    # -------------------------------------------------------------------------
    try:
        # S√©curisation des types
        s_diag = float(eval_json.get("score_diagnostic", 0))
        s_ther = float(eval_json.get("score_therapeutique", 0))
        s_dem = float(eval_json.get("score_demarche", 0))
        
        # Clamp des notes (au cas o√π l'IA note sur 20 au lieu de 10)
        s_diag = min(10, max(0, s_diag))
        s_ther = min(5, max(0, s_ther))
        s_dem = min(5, max(0, s_dem))
        
        total = s_diag + s_ther + s_dem
        
        logger.info(f"   üèÜ [{eval_id}] Verdict rendu : {total}/20")
        logger.debug(f"      D√©tails : Diag={s_diag}/10, Ther={s_ther}/5, Dem={s_dem}/5")
        logger.debug(f"      Feedback : {eval_json.get('feedback_global', '')[:100]}...")

        result_obj = schemas.simulation.EvaluationResult(
            score_diagnostic=s_diag,
            score_therapeutique=s_ther,
            score_demarche=s_dem,
            score_total=total
        )
        
        return result_obj, eval_json.get("feedback_global", "√âvaluation compl√©t√©e."), eval_json.get("recommendation_next_step", "Continuer.")

    except Exception as e:
        logger.error(f"   ‚ùå [{eval_id}] Erreur lecture verdict : {e}")
        logger.debug(f"      JSON re√ßu : {eval_json}")
        
        # Fallback pour ne pas bloquer l'UI
        return schemas.simulation.EvaluationResult(
            score_diagnostic=0, score_therapeutique=0, score_demarche=0, score_total=0
        ), "Erreur technique lors de l'√©valuation automatique. Vos r√©ponses ont √©t√© enregistr√©es.", "Veuillez contacter l'administrateur."

def generate_hint(case: models.ClinicalCase, session_history: List[str], hint_level: int) -> Tuple[str, str]:
    """
    G√©n√®re un indice.
    """
    logger.info(f"üí° [AI-TUTOR] Indice niveau {hint_level}")
    
    prompt = f"""
ROLE: Tuteur m√©dical.
CONTEXTE: Cas de {case.pathologie_principale.nom_fr}.
NIVEAU AIDE: {hint_level}/3.
HISTORIQUE: {str(session_history)[-1000:]}

Donne un indice p√©dagogique JSON : {{ "hint_type": "...", "content": "..." }}
"""
    res = _call_openrouter_api(prompt, json_mode=True, task_type=AiTaskType.HINT_GENERATION)
    if isinstance(res, dict):
        return res.get("hint_type", "info"), res.get("content", "Analysez les sympt√¥mes.")
    return "info", "Continuez."